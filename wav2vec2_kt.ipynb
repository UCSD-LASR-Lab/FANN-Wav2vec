{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cb8cbb82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4885/3487006908.py:7: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"librosa\")\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import torchaudio\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, Wav2Vec2CTCTokenizer\n",
    "torchaudio.set_audio_backend(\"librosa\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9568669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(file_path, processor):\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != processor.feature_extractor.sampling_rate:\n",
    "        waveform = torchaudio.transforms.Resample(\n",
    "            orig_freq= sample_rate, \n",
    "            new_freq= processor.feature_extractor.sampling_rate\n",
    "        )(waveform)\n",
    " \n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f584819a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "base_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "base_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "large_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "large_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d35086f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def general_predict_and_probability(file_path, processor, model):\n",
    "    waveform = process_audio(file_path, processor)\n",
    "    inputs = processor(\n",
    "        waveform,\n",
    "        return_tensors= 'pt',\n",
    "        sampling_rate= processor.feature_extractor.sampling_rate\n",
    "    )\n",
    "    \n",
    "    inputs = inputs.input_values.squeeze(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values= inputs).logits \n",
    "        \n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_word = processor.decode(predicted_ids[0])\n",
    "    softmax_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "    max_probs, _ = torch.max(softmax_probs, dim=-1) \n",
    "    \n",
    "#     print(logits.shape)\n",
    "#     print(logits)\n",
    "#     print(softmax_probs)\n",
    "#     print(softmax_probs.shape)\n",
    "#     top_probs, top_indices = torch.topk(softmax_probs, 2, dim=-1)\n",
    "#     print(top_probs.shape)\n",
    "#     print(processor.decode(top_indices[:, :, 1].squeeze()))\n",
    "\n",
    "    return predicted_word, max_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82b4be34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CHIP',\n",
       " tensor([[0.9985, 0.9986, 0.9982, 0.9973, 0.9950, 0.9926, 0.9946, 0.8415, 0.9932,\n",
       "          0.7393, 0.9329, 0.9832, 0.9813, 0.9873, 0.9923, 0.9911, 0.4034, 0.6599,\n",
       "          0.9232, 0.9854, 0.9957, 0.6874, 0.9595, 0.8922, 0.8735, 0.9207, 0.9622,\n",
       "          0.6285, 0.6359, 0.7261, 0.9979, 0.9979, 0.9991, 0.9993, 0.9988, 0.9991,\n",
       "          0.9992, 0.9992, 0.9990, 0.9992, 0.9992, 0.9993, 0.9989, 0.9993, 0.9989,\n",
       "          0.9994, 0.9989, 0.9989, 0.9995, 0.9994, 0.9989, 0.9991]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chip_path = \"/home/cogsci-lasrlab/Desktop/KT2 exptal audio/K2a0_chip.wav\"\n",
    "\n",
    "base_word, base_prob = general_predict_and_probability(\n",
    "    file_path= chip_path,\n",
    "    processor= base_processor,\n",
    "    model= base_model\n",
    ")\n",
    "\n",
    "base_word, base_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8241a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('CHAP',\n",
       " tensor([[0.9999, 0.9999, 1.0000, 0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 0.9780, 0.9928, 0.9997, 0.9994, 0.9985, 0.8239, 0.9994,\n",
       "          0.9991, 0.9867, 0.9324, 0.9775, 0.9979, 0.9987, 0.9993, 0.9997, 0.9997,\n",
       "          0.9995, 0.9984, 0.9145, 1.0000, 0.9987, 0.9990, 0.9992, 0.9986, 0.9874,\n",
       "          0.9999, 0.9986, 0.9986, 0.9995, 0.8929, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_word, large_prob = general_predict_and_probability(\n",
    "    file_path= chip_path,\n",
    "    processor= large_processor,\n",
    "    model= large_model\n",
    ")\n",
    "\n",
    "large_word, large_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f8f11d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FAON',\n",
       " tensor([[0.9997, 0.9997, 0.9997, 0.9997, 0.4753, 0.8813, 0.9283, 0.9959, 0.9973,\n",
       "          0.9575, 0.7329, 0.7121, 0.8088, 0.9664, 0.9863, 0.9896, 0.9836, 0.9599,\n",
       "          0.8962, 0.9889, 0.9947, 0.9948, 0.9944, 0.9947, 0.9966, 0.6045, 0.9848,\n",
       "          0.9932, 0.9608, 0.9471, 0.8795, 0.9249, 0.9784, 0.9900, 0.9979, 0.9460,\n",
       "          0.9999, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9999, 1.0000, 0.9999,\n",
       "          0.9999, 1.0000, 0.9999, 0.9999, 0.9999, 0.9999]]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fan_path = \"/home/cogsci-lasrlab/Desktop/KT2 exptal audio/K2a0_fan.wav\"\n",
    "\n",
    "base_word, base_prob = general_predict_and_probability(\n",
    "    file_path= fan_path,\n",
    "    processor= base_processor,\n",
    "    model= base_model\n",
    ")\n",
    "\n",
    "base_word, base_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c785b8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('FELL ON',\n",
       " tensor([[0.9999, 0.9999, 0.9998, 0.9997, 0.9985, 0.9413, 0.6250, 0.7739, 0.9917,\n",
       "          0.6135, 0.9964, 0.9888, 0.7475, 0.9287, 0.5199, 0.8009, 0.9006, 0.6589,\n",
       "          0.8981, 0.9890, 0.9909, 0.9995, 0.9550, 0.9958, 0.9905, 0.9990, 0.9994,\n",
       "          0.9893, 0.9916, 0.9668, 0.9905, 0.9960, 0.9789, 0.9969, 0.9714, 0.9427,\n",
       "          0.9621, 0.9917, 0.9996, 0.9628, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
       "          1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_word, large_prob = general_predict_and_probability(\n",
    "    file_path= fan_path,\n",
    "    processor= large_processor,\n",
    "    model= large_model\n",
    ")\n",
    "\n",
    "large_word, large_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecd1332a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BRAN',\n",
       " tensor([[0.4337, 0.9933, 0.9902, 0.8525, 0.9865, 0.9855, 0.9875, 0.9901, 0.9928,\n",
       "          0.9890, 0.8744, 0.9209, 0.9610, 0.9522, 0.9898, 0.6860, 0.9006, 0.9794,\n",
       "          0.9841, 0.9308, 0.9963, 0.9994, 0.9996, 0.9985, 0.9985, 0.9985]]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_path = \"/home/cogsci-lasrlab/Desktop/KT2 exptal audio/K2a0_run.wav\"\n",
    "\n",
    "base_word, base_prob = general_predict_and_probability(\n",
    "    file_path= run_path,\n",
    "    processor= base_processor,\n",
    "    model= base_model\n",
    ")\n",
    "\n",
    "base_word, base_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca73f555",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('RAN',\n",
       " tensor([[0.9990, 0.9969, 0.9818, 0.9485, 0.8027, 0.9983, 0.9996, 0.7243, 0.9942,\n",
       "          0.9910, 0.9751, 0.9549, 0.9612, 0.8069, 0.9590, 0.9780, 0.9704, 0.9787,\n",
       "          0.9752, 0.9729, 0.9682, 0.5553, 0.9523, 0.9838, 0.8307, 0.9990]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_word, large_prob = general_predict_and_probability(\n",
    "    file_path= run_path,\n",
    "    processor= large_processor,\n",
    "    model= large_model\n",
    ")\n",
    "\n",
    "large_word, large_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81be80da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_predict_and_probability(file_path, processor, model):\n",
    "    waveform = process_audio(file_path, processor)\n",
    "    inputs = processor(\n",
    "        waveform,\n",
    "        return_tensors= 'pt',\n",
    "        sampling_rate= processor.feature_extractor.sampling_rate\n",
    "    )\n",
    "    \n",
    "    inputs = inputs.input_values.squeeze(1)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values= inputs).logits \n",
    "        \n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    predicted_word = processor.decode(predicted_ids[0])\n",
    "    softmax_probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "    return predicted_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "95ee6735",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "chip_vocab_path = '/home/cogsci-lasrlab/Documents/FANN/vocab/4_chip.json'\n",
    "chip_custom_tokenizer = Wav2Vec2CTCTokenizer(chip_vocab_path)\n",
    "\n",
    "base_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "base_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\", tokenizer = chip_custom_tokenizer)\n",
    "\n",
    "large_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "large_processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\", tokenizer = chip_custom_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "57affe8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_processor.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2e62e400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 52, 32])\n",
      "tensor([[[  8.5268, -21.4147, -21.1832,  ...,  -4.8406,  -4.9408,  -6.1256],\n",
      "         [  8.6224, -21.6634, -21.4232,  ...,  -4.5749,  -5.1386,  -5.7544],\n",
      "         [  8.6859, -21.8377, -21.6726,  ...,  -4.1219,  -5.3921,  -5.3512],\n",
      "         ...,\n",
      "         [  9.3211, -22.3689, -22.0702,  ...,  -5.2648,  -6.6861,  -4.9645],\n",
      "         [  8.6201, -19.5929, -19.3630,  ...,  -3.6756,  -4.7550,  -3.4002],\n",
      "         [  9.1385, -22.5031, -22.2302,  ...,  -5.4515,  -6.6649,  -5.0894]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chip<unk>chip<unk>chip<unk>chip<unk>chip<s>chip'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chip = predict_word_and_probability(\n",
    "    file_path= \"/home/cogsci-lasrlab/Desktop/KT2 exptal audio/K2a0_chip.wav\", \n",
    "    processor= base_processor, \n",
    "    model= base_model\n",
    ")\n",
    "\n",
    "chip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "325d935c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 52, 32])\n",
      "tensor([[[ 11.9968, -28.1995, -28.2689,  ...,  -4.3413,  -3.7289,  -6.7600],\n",
      "         [ 11.9771, -28.1132, -28.1832,  ...,  -4.3190,  -3.9699,  -6.6605],\n",
      "         [ 12.5852, -29.3019, -29.3424,  ...,  -4.4923,  -4.3289,  -6.6859],\n",
      "         ...,\n",
      "         [ 14.9882, -33.8904, -33.7044,  ...,  -5.8607,  -5.2790,  -7.8795],\n",
      "         [ 14.8117, -33.4340, -33.2653,  ...,  -5.6307,  -5.1198,  -7.7124],\n",
      "         [ 14.9083, -33.6068, -33.4321,  ...,  -5.6873,  -5.2208,  -7.7465]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'chip<unk>chipchip<unk>chip<s>chip'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chip = predict_word_and_probability(\n",
    "    file_path= \"/home/cogsci-lasrlab/Desktop/KT2 exptal audio/K2a0_chip.wav\", \n",
    "    processor= large_processor, \n",
    "    model= large_model\n",
    ")\n",
    "\n",
    "chip"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (my_venv)",
   "language": "python",
   "name": "fann"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
