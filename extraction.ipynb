{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75108dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "import torch \n",
    "import librosa \n",
    "import scipy.signal as sig \n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, WhisperProcessor, WhisperForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c0a1c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# wv2_base_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "# wv2_base_proc = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# wv2_large_model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-large-960h\")\n",
    "# wv2_large_proc = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-large-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9140b734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "whisper_large_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3-turbo\")\n",
    "whisper_large_proc = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db8d124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chip_wav = '/home/cogsci-lasrlab/Documents/FANN/K1_wav_kid/K1AM166participant_chip.wav'\n",
    "ship_wav = '/home/cogsci-lasrlab/Documents/FANN/K1_wav_kid/K1AM166participant_ship.wav'\n",
    "hotwords = ['chip', 'ship', 'house', 'towel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6e0c8d",
   "metadata": {},
   "source": [
    "### Extracting Whisper Probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e24b53f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_vocab_dict = whisper_large_proc.tokenizer.get_vocab()\n",
    "sorted_whisper_vocab_dict = {k.replace('Ġ', ' '): v for k, v in sorted(whisper_vocab_dict.items(), key = lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f964721e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7524\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "new_dict = {}\n",
    "for k, v in sorted_whisper_vocab_dict.items():\n",
    "    for h in hotwords: \n",
    "        if h in k or h.upper() in k.upper() or h.lower() in k.lower():\n",
    "            count += 1 \n",
    "            new_dict[k] = v \n",
    "            \n",
    "        if len(k) < 2 or '<|' in k: \n",
    "            count += 1 \n",
    "            new_dict[k] = v\n",
    "            \n",
    "print(count)\n",
    "\n",
    "ind_list = [v for v in new_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3b760c7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " '¡': 94,\n",
       " '¢': 95,\n",
       " '£': 96,\n",
       " '¤': 97,\n",
       " '¥': 98,\n",
       " '¦': 99,\n",
       " '§': 100,\n",
       " '¨': 101,\n",
       " '©': 102,\n",
       " 'ª': 103,\n",
       " '«': 104,\n",
       " '¬': 105,\n",
       " '®': 106,\n",
       " '¯': 107,\n",
       " '°': 108,\n",
       " '±': 109,\n",
       " '²': 110,\n",
       " '³': 111,\n",
       " '´': 112,\n",
       " 'µ': 113,\n",
       " '¶': 114,\n",
       " '·': 115,\n",
       " '¸': 116,\n",
       " '¹': 117,\n",
       " 'º': 118,\n",
       " '»': 119,\n",
       " '¼': 120,\n",
       " '½': 121,\n",
       " '¾': 122,\n",
       " '¿': 123,\n",
       " 'À': 124,\n",
       " 'Á': 125,\n",
       " 'Â': 126,\n",
       " 'Ã': 127,\n",
       " 'Ä': 128,\n",
       " 'Å': 129,\n",
       " 'Æ': 130,\n",
       " 'Ç': 131,\n",
       " 'È': 132,\n",
       " 'É': 133,\n",
       " 'Ê': 134,\n",
       " 'Ë': 135,\n",
       " 'Ì': 136,\n",
       " 'Í': 137,\n",
       " 'Î': 138,\n",
       " 'Ï': 139,\n",
       " 'Ð': 140,\n",
       " 'Ñ': 141,\n",
       " 'Ò': 142,\n",
       " 'Ó': 143,\n",
       " 'Ô': 144,\n",
       " 'Õ': 145,\n",
       " 'Ö': 146,\n",
       " '×': 147,\n",
       " 'Ø': 148,\n",
       " 'Ù': 149,\n",
       " 'Ú': 150,\n",
       " 'Û': 151,\n",
       " 'Ü': 152,\n",
       " 'Ý': 153,\n",
       " 'Þ': 154,\n",
       " 'ß': 155,\n",
       " 'à': 156,\n",
       " 'á': 157,\n",
       " 'â': 158,\n",
       " 'ã': 159,\n",
       " 'ä': 160,\n",
       " 'å': 161,\n",
       " 'æ': 162,\n",
       " 'ç': 163,\n",
       " 'è': 164,\n",
       " 'é': 165,\n",
       " 'ê': 166,\n",
       " 'ë': 167,\n",
       " 'ì': 168,\n",
       " 'í': 169,\n",
       " 'î': 170,\n",
       " 'ï': 171,\n",
       " 'ð': 172,\n",
       " 'ñ': 173,\n",
       " 'ò': 174,\n",
       " 'ó': 175,\n",
       " 'ô': 176,\n",
       " 'õ': 177,\n",
       " 'ö': 178,\n",
       " '÷': 179,\n",
       " 'ø': 180,\n",
       " 'ù': 181,\n",
       " 'ú': 182,\n",
       " 'û': 183,\n",
       " 'ü': 184,\n",
       " 'ý': 185,\n",
       " 'þ': 186,\n",
       " 'ÿ': 187,\n",
       " 'Ā': 188,\n",
       " 'ā': 189,\n",
       " 'Ă': 190,\n",
       " 'ă': 191,\n",
       " 'Ą': 192,\n",
       " 'ą': 193,\n",
       " 'Ć': 194,\n",
       " 'ć': 195,\n",
       " 'Ĉ': 196,\n",
       " 'ĉ': 197,\n",
       " 'Ċ': 198,\n",
       " 'ċ': 199,\n",
       " 'Č': 200,\n",
       " 'č': 201,\n",
       " 'Ď': 202,\n",
       " 'ď': 203,\n",
       " 'Đ': 204,\n",
       " 'đ': 205,\n",
       " 'Ē': 206,\n",
       " 'ē': 207,\n",
       " 'Ĕ': 208,\n",
       " 'ĕ': 209,\n",
       " 'Ė': 210,\n",
       " 'ė': 211,\n",
       " 'Ę': 212,\n",
       " 'ę': 213,\n",
       " 'Ě': 214,\n",
       " 'ě': 215,\n",
       " 'Ĝ': 216,\n",
       " 'ĝ': 217,\n",
       " 'Ğ': 218,\n",
       " 'ğ': 219,\n",
       " ' ': 220,\n",
       " 'ġ': 221,\n",
       " 'Ģ': 222,\n",
       " 'ģ': 223,\n",
       " 'Ĥ': 224,\n",
       " 'ĥ': 225,\n",
       " 'Ħ': 226,\n",
       " 'ħ': 227,\n",
       " 'Ĩ': 228,\n",
       " 'ĩ': 229,\n",
       " 'Ī': 230,\n",
       " 'ī': 231,\n",
       " 'Ĭ': 232,\n",
       " 'ĭ': 233,\n",
       " 'Į': 234,\n",
       " 'į': 235,\n",
       " 'İ': 236,\n",
       " 'ı': 237,\n",
       " 'Ĳ': 238,\n",
       " 'ĳ': 239,\n",
       " 'Ĵ': 240,\n",
       " 'ĵ': 241,\n",
       " 'Ķ': 242,\n",
       " 'ķ': 243,\n",
       " 'ĸ': 244,\n",
       " 'Ĺ': 245,\n",
       " 'ĺ': 246,\n",
       " 'Ļ': 247,\n",
       " 'ļ': 248,\n",
       " 'Ľ': 249,\n",
       " 'ľ': 250,\n",
       " 'Ŀ': 251,\n",
       " 'ŀ': 252,\n",
       " 'Ł': 253,\n",
       " 'ł': 254,\n",
       " 'Ń': 255,\n",
       " ' house': 1782,\n",
       " ' relationship': 2480,\n",
       " ' House': 4928,\n",
       " 'ship': 5278,\n",
       " ' ship': 5374,\n",
       " ' leadership': 5848,\n",
       " ' relationships': 6159,\n",
       " 'house': 6410,\n",
       " ' houses': 8078,\n",
       " ' household': 9888,\n",
       " ' worship': 9965,\n",
       " ' partnership': 9982,\n",
       " ' chip': 11409,\n",
       " ' ships': 11434,\n",
       " ' chips': 11583,\n",
       " ' friendship': 13216,\n",
       " ' shipping': 14122,\n",
       " 'orship': 14752,\n",
       " ' ownership': 15279,\n",
       " ' towel': 15755,\n",
       " ' scholarship': 16178,\n",
       " ' membership': 16560,\n",
       " ' internship': 16861,\n",
       " ' partnerships': 18245,\n",
       " ' championship': 19070,\n",
       " ' greenhouse': 22126,\n",
       " ' warehouse': 22244,\n",
       " ' households': 22850,\n",
       " ' citizenship': 23808,\n",
       " ' hardship': 24172,\n",
       " ' Championship': 24310,\n",
       " ' fellowship': 24989,\n",
       " ' shipped': 25312,\n",
       " ' entrepreneurship': 26582,\n",
       " 'anship': 27140,\n",
       " ' scholarships': 28474,\n",
       " 'houses': 29578,\n",
       " ' Chip': 29751,\n",
       " ' friendships': 30003,\n",
       " ' flagship': 30400,\n",
       " ' Leadership': 30577,\n",
       " ' towels': 32819,\n",
       " ' internships': 35712,\n",
       " ' housed': 36084,\n",
       " ' Ship': 38407,\n",
       " ' spaceship': 39185,\n",
       " ' mentorship': 40422,\n",
       " ' censorship': 40985,\n",
       " ' hardships': 41351,\n",
       " ' championships': 41433,\n",
       " 'manship': 42782,\n",
       " ' sponsorship': 42922,\n",
       " ' dictatorship': 44349,\n",
       " ' Championships': 46917,\n",
       " ' apprenticeship': 47070,\n",
       " ' lighthouse': 47481,\n",
       " ' housekeeping': 48033,\n",
       " ' Partnership': 49589,\n",
       " ' shipment': 49991,\n",
       " ' stewardship': 50092,\n",
       " '': 50256,\n",
       " '<|endoftext|>': 50257,\n",
       " '<|startoftranscript|>': 50258,\n",
       " '<|en|>': 50259,\n",
       " '<|zh|>': 50260,\n",
       " '<|de|>': 50261,\n",
       " '<|es|>': 50262,\n",
       " '<|ru|>': 50263,\n",
       " '<|ko|>': 50264,\n",
       " '<|fr|>': 50265,\n",
       " '<|ja|>': 50266,\n",
       " '<|pt|>': 50267,\n",
       " '<|tr|>': 50268,\n",
       " '<|pl|>': 50269,\n",
       " '<|ca|>': 50270,\n",
       " '<|nl|>': 50271,\n",
       " '<|ar|>': 50272,\n",
       " '<|sv|>': 50273,\n",
       " '<|it|>': 50274,\n",
       " '<|id|>': 50275,\n",
       " '<|hi|>': 50276,\n",
       " '<|fi|>': 50277,\n",
       " '<|vi|>': 50278,\n",
       " '<|he|>': 50279,\n",
       " '<|uk|>': 50280,\n",
       " '<|el|>': 50281,\n",
       " '<|ms|>': 50282,\n",
       " '<|cs|>': 50283,\n",
       " '<|ro|>': 50284,\n",
       " '<|da|>': 50285,\n",
       " '<|hu|>': 50286,\n",
       " '<|ta|>': 50287,\n",
       " '<|no|>': 50288,\n",
       " '<|th|>': 50289,\n",
       " '<|ur|>': 50290,\n",
       " '<|hr|>': 50291,\n",
       " '<|bg|>': 50292,\n",
       " '<|lt|>': 50293,\n",
       " '<|la|>': 50294,\n",
       " '<|mi|>': 50295,\n",
       " '<|ml|>': 50296,\n",
       " '<|cy|>': 50297,\n",
       " '<|sk|>': 50298,\n",
       " '<|te|>': 50299,\n",
       " '<|fa|>': 50300,\n",
       " '<|lv|>': 50301,\n",
       " '<|bn|>': 50302,\n",
       " '<|sr|>': 50303,\n",
       " '<|az|>': 50304,\n",
       " '<|sl|>': 50305,\n",
       " '<|kn|>': 50306,\n",
       " '<|et|>': 50307,\n",
       " '<|mk|>': 50308,\n",
       " '<|br|>': 50309,\n",
       " '<|eu|>': 50310,\n",
       " '<|is|>': 50311,\n",
       " '<|hy|>': 50312,\n",
       " '<|ne|>': 50313,\n",
       " '<|mn|>': 50314,\n",
       " '<|bs|>': 50315,\n",
       " '<|kk|>': 50316,\n",
       " '<|sq|>': 50317,\n",
       " '<|sw|>': 50318,\n",
       " '<|gl|>': 50319,\n",
       " '<|mr|>': 50320,\n",
       " '<|pa|>': 50321,\n",
       " '<|si|>': 50322,\n",
       " '<|km|>': 50323,\n",
       " '<|sn|>': 50324,\n",
       " '<|yo|>': 50325,\n",
       " '<|so|>': 50326,\n",
       " '<|af|>': 50327,\n",
       " '<|oc|>': 50328,\n",
       " '<|ka|>': 50329,\n",
       " '<|be|>': 50330,\n",
       " '<|tg|>': 50331,\n",
       " '<|sd|>': 50332,\n",
       " '<|gu|>': 50333,\n",
       " '<|am|>': 50334,\n",
       " '<|yi|>': 50335,\n",
       " '<|lo|>': 50336,\n",
       " '<|uz|>': 50337,\n",
       " '<|fo|>': 50338,\n",
       " '<|ht|>': 50339,\n",
       " '<|ps|>': 50340,\n",
       " '<|tk|>': 50341,\n",
       " '<|nn|>': 50342,\n",
       " '<|mt|>': 50343,\n",
       " '<|sa|>': 50344,\n",
       " '<|lb|>': 50345,\n",
       " '<|my|>': 50346,\n",
       " '<|bo|>': 50347,\n",
       " '<|tl|>': 50348,\n",
       " '<|mg|>': 50349,\n",
       " '<|as|>': 50350,\n",
       " '<|tt|>': 50351,\n",
       " '<|haw|>': 50352,\n",
       " '<|ln|>': 50353,\n",
       " '<|ha|>': 50354,\n",
       " '<|ba|>': 50355,\n",
       " '<|jw|>': 50356,\n",
       " '<|su|>': 50357,\n",
       " '<|yue|>': 50358,\n",
       " '<|translate|>': 50359,\n",
       " '<|transcribe|>': 50360,\n",
       " '<|startoflm|>': 50361,\n",
       " '<|startofprev|>': 50362,\n",
       " '<|nospeech|>': 50363,\n",
       " '<|notimestamps|>': 50364,\n",
       " '<|0.00|>': 50365,\n",
       " '<|0.02|>': 50366,\n",
       " '<|0.04|>': 50367,\n",
       " '<|0.06|>': 50368,\n",
       " '<|0.08|>': 50369,\n",
       " '<|0.10|>': 50370,\n",
       " '<|0.12|>': 50371,\n",
       " '<|0.14|>': 50372,\n",
       " '<|0.16|>': 50373,\n",
       " '<|0.18|>': 50374,\n",
       " '<|0.20|>': 50375,\n",
       " '<|0.22|>': 50376,\n",
       " '<|0.24|>': 50377,\n",
       " '<|0.26|>': 50378,\n",
       " '<|0.28|>': 50379,\n",
       " '<|0.30|>': 50380,\n",
       " '<|0.32|>': 50381,\n",
       " '<|0.34|>': 50382,\n",
       " '<|0.36|>': 50383,\n",
       " '<|0.38|>': 50384,\n",
       " '<|0.40|>': 50385,\n",
       " '<|0.42|>': 50386,\n",
       " '<|0.44|>': 50387,\n",
       " '<|0.46|>': 50388,\n",
       " '<|0.48|>': 50389,\n",
       " '<|0.50|>': 50390,\n",
       " '<|0.52|>': 50391,\n",
       " '<|0.54|>': 50392,\n",
       " '<|0.56|>': 50393,\n",
       " '<|0.58|>': 50394,\n",
       " '<|0.60|>': 50395,\n",
       " '<|0.62|>': 50396,\n",
       " '<|0.64|>': 50397,\n",
       " '<|0.66|>': 50398,\n",
       " '<|0.68|>': 50399,\n",
       " '<|0.70|>': 50400,\n",
       " '<|0.72|>': 50401,\n",
       " '<|0.74|>': 50402,\n",
       " '<|0.76|>': 50403,\n",
       " '<|0.78|>': 50404,\n",
       " '<|0.80|>': 50405,\n",
       " '<|0.82|>': 50406,\n",
       " '<|0.84|>': 50407,\n",
       " '<|0.86|>': 50408,\n",
       " '<|0.88|>': 50409,\n",
       " '<|0.90|>': 50410,\n",
       " '<|0.92|>': 50411,\n",
       " '<|0.94|>': 50412,\n",
       " '<|0.96|>': 50413,\n",
       " '<|0.98|>': 50414,\n",
       " '<|1.00|>': 50415,\n",
       " '<|1.02|>': 50416,\n",
       " '<|1.04|>': 50417,\n",
       " '<|1.06|>': 50418,\n",
       " '<|1.08|>': 50419,\n",
       " '<|1.10|>': 50420,\n",
       " '<|1.12|>': 50421,\n",
       " '<|1.14|>': 50422,\n",
       " '<|1.16|>': 50423,\n",
       " '<|1.18|>': 50424,\n",
       " '<|1.20|>': 50425,\n",
       " '<|1.22|>': 50426,\n",
       " '<|1.24|>': 50427,\n",
       " '<|1.26|>': 50428,\n",
       " '<|1.28|>': 50429,\n",
       " '<|1.30|>': 50430,\n",
       " '<|1.32|>': 50431,\n",
       " '<|1.34|>': 50432,\n",
       " '<|1.36|>': 50433,\n",
       " '<|1.38|>': 50434,\n",
       " '<|1.40|>': 50435,\n",
       " '<|1.42|>': 50436,\n",
       " '<|1.44|>': 50437,\n",
       " '<|1.46|>': 50438,\n",
       " '<|1.48|>': 50439,\n",
       " '<|1.50|>': 50440,\n",
       " '<|1.52|>': 50441,\n",
       " '<|1.54|>': 50442,\n",
       " '<|1.56|>': 50443,\n",
       " '<|1.58|>': 50444,\n",
       " '<|1.60|>': 50445,\n",
       " '<|1.62|>': 50446,\n",
       " '<|1.64|>': 50447,\n",
       " '<|1.66|>': 50448,\n",
       " '<|1.68|>': 50449,\n",
       " '<|1.70|>': 50450,\n",
       " '<|1.72|>': 50451,\n",
       " '<|1.74|>': 50452,\n",
       " '<|1.76|>': 50453,\n",
       " '<|1.78|>': 50454,\n",
       " '<|1.80|>': 50455,\n",
       " '<|1.82|>': 50456,\n",
       " '<|1.84|>': 50457,\n",
       " '<|1.86|>': 50458,\n",
       " '<|1.88|>': 50459,\n",
       " '<|1.90|>': 50460,\n",
       " '<|1.92|>': 50461,\n",
       " '<|1.94|>': 50462,\n",
       " '<|1.96|>': 50463,\n",
       " '<|1.98|>': 50464,\n",
       " '<|2.00|>': 50465,\n",
       " '<|2.02|>': 50466,\n",
       " '<|2.04|>': 50467,\n",
       " '<|2.06|>': 50468,\n",
       " '<|2.08|>': 50469,\n",
       " '<|2.10|>': 50470,\n",
       " '<|2.12|>': 50471,\n",
       " '<|2.14|>': 50472,\n",
       " '<|2.16|>': 50473,\n",
       " '<|2.18|>': 50474,\n",
       " '<|2.20|>': 50475,\n",
       " '<|2.22|>': 50476,\n",
       " '<|2.24|>': 50477,\n",
       " '<|2.26|>': 50478,\n",
       " '<|2.28|>': 50479,\n",
       " '<|2.30|>': 50480,\n",
       " '<|2.32|>': 50481,\n",
       " '<|2.34|>': 50482,\n",
       " '<|2.36|>': 50483,\n",
       " '<|2.38|>': 50484,\n",
       " '<|2.40|>': 50485,\n",
       " '<|2.42|>': 50486,\n",
       " '<|2.44|>': 50487,\n",
       " '<|2.46|>': 50488,\n",
       " '<|2.48|>': 50489,\n",
       " '<|2.50|>': 50490,\n",
       " '<|2.52|>': 50491,\n",
       " '<|2.54|>': 50492,\n",
       " '<|2.56|>': 50493,\n",
       " '<|2.58|>': 50494,\n",
       " '<|2.60|>': 50495,\n",
       " '<|2.62|>': 50496,\n",
       " '<|2.64|>': 50497,\n",
       " '<|2.66|>': 50498,\n",
       " '<|2.68|>': 50499,\n",
       " '<|2.70|>': 50500,\n",
       " '<|2.72|>': 50501,\n",
       " '<|2.74|>': 50502,\n",
       " '<|2.76|>': 50503,\n",
       " '<|2.78|>': 50504,\n",
       " '<|2.80|>': 50505,\n",
       " '<|2.82|>': 50506,\n",
       " '<|2.84|>': 50507,\n",
       " '<|2.86|>': 50508,\n",
       " '<|2.88|>': 50509,\n",
       " '<|2.90|>': 50510,\n",
       " '<|2.92|>': 50511,\n",
       " '<|2.94|>': 50512,\n",
       " '<|2.96|>': 50513,\n",
       " '<|2.98|>': 50514,\n",
       " '<|3.00|>': 50515,\n",
       " '<|3.02|>': 50516,\n",
       " '<|3.04|>': 50517,\n",
       " '<|3.06|>': 50518,\n",
       " '<|3.08|>': 50519,\n",
       " '<|3.10|>': 50520,\n",
       " '<|3.12|>': 50521,\n",
       " '<|3.14|>': 50522,\n",
       " '<|3.16|>': 50523,\n",
       " '<|3.18|>': 50524,\n",
       " '<|3.20|>': 50525,\n",
       " '<|3.22|>': 50526,\n",
       " '<|3.24|>': 50527,\n",
       " '<|3.26|>': 50528,\n",
       " '<|3.28|>': 50529,\n",
       " '<|3.30|>': 50530,\n",
       " '<|3.32|>': 50531,\n",
       " '<|3.34|>': 50532,\n",
       " '<|3.36|>': 50533,\n",
       " '<|3.38|>': 50534,\n",
       " '<|3.40|>': 50535,\n",
       " '<|3.42|>': 50536,\n",
       " '<|3.44|>': 50537,\n",
       " '<|3.46|>': 50538,\n",
       " '<|3.48|>': 50539,\n",
       " '<|3.50|>': 50540,\n",
       " '<|3.52|>': 50541,\n",
       " '<|3.54|>': 50542,\n",
       " '<|3.56|>': 50543,\n",
       " '<|3.58|>': 50544,\n",
       " '<|3.60|>': 50545,\n",
       " '<|3.62|>': 50546,\n",
       " '<|3.64|>': 50547,\n",
       " '<|3.66|>': 50548,\n",
       " '<|3.68|>': 50549,\n",
       " '<|3.70|>': 50550,\n",
       " '<|3.72|>': 50551,\n",
       " '<|3.74|>': 50552,\n",
       " '<|3.76|>': 50553,\n",
       " '<|3.78|>': 50554,\n",
       " '<|3.80|>': 50555,\n",
       " '<|3.82|>': 50556,\n",
       " '<|3.84|>': 50557,\n",
       " '<|3.86|>': 50558,\n",
       " '<|3.88|>': 50559,\n",
       " '<|3.90|>': 50560,\n",
       " '<|3.92|>': 50561,\n",
       " '<|3.94|>': 50562,\n",
       " '<|3.96|>': 50563,\n",
       " '<|3.98|>': 50564,\n",
       " '<|4.00|>': 50565,\n",
       " '<|4.02|>': 50566,\n",
       " '<|4.04|>': 50567,\n",
       " '<|4.06|>': 50568,\n",
       " '<|4.08|>': 50569,\n",
       " '<|4.10|>': 50570,\n",
       " '<|4.12|>': 50571,\n",
       " '<|4.14|>': 50572,\n",
       " '<|4.16|>': 50573,\n",
       " '<|4.18|>': 50574,\n",
       " '<|4.20|>': 50575,\n",
       " '<|4.22|>': 50576,\n",
       " '<|4.24|>': 50577,\n",
       " '<|4.26|>': 50578,\n",
       " '<|4.28|>': 50579,\n",
       " '<|4.30|>': 50580,\n",
       " '<|4.32|>': 50581,\n",
       " '<|4.34|>': 50582,\n",
       " '<|4.36|>': 50583,\n",
       " '<|4.38|>': 50584,\n",
       " '<|4.40|>': 50585,\n",
       " '<|4.42|>': 50586,\n",
       " '<|4.44|>': 50587,\n",
       " '<|4.46|>': 50588,\n",
       " '<|4.48|>': 50589,\n",
       " '<|4.50|>': 50590,\n",
       " '<|4.52|>': 50591,\n",
       " '<|4.54|>': 50592,\n",
       " '<|4.56|>': 50593,\n",
       " '<|4.58|>': 50594,\n",
       " '<|4.60|>': 50595,\n",
       " '<|4.62|>': 50596,\n",
       " '<|4.64|>': 50597,\n",
       " '<|4.66|>': 50598,\n",
       " '<|4.68|>': 50599,\n",
       " '<|4.70|>': 50600,\n",
       " '<|4.72|>': 50601,\n",
       " '<|4.74|>': 50602,\n",
       " '<|4.76|>': 50603,\n",
       " '<|4.78|>': 50604,\n",
       " '<|4.80|>': 50605,\n",
       " '<|4.82|>': 50606,\n",
       " '<|4.84|>': 50607,\n",
       " '<|4.86|>': 50608,\n",
       " '<|4.88|>': 50609,\n",
       " '<|4.90|>': 50610,\n",
       " '<|4.92|>': 50611,\n",
       " '<|4.94|>': 50612,\n",
       " '<|4.96|>': 50613,\n",
       " '<|4.98|>': 50614,\n",
       " '<|5.00|>': 50615,\n",
       " '<|5.02|>': 50616,\n",
       " '<|5.04|>': 50617,\n",
       " '<|5.06|>': 50618,\n",
       " '<|5.08|>': 50619,\n",
       " '<|5.10|>': 50620,\n",
       " '<|5.12|>': 50621,\n",
       " '<|5.14|>': 50622,\n",
       " '<|5.16|>': 50623,\n",
       " '<|5.18|>': 50624,\n",
       " '<|5.20|>': 50625,\n",
       " '<|5.22|>': 50626,\n",
       " '<|5.24|>': 50627,\n",
       " '<|5.26|>': 50628,\n",
       " '<|5.28|>': 50629,\n",
       " '<|5.30|>': 50630,\n",
       " '<|5.32|>': 50631,\n",
       " '<|5.34|>': 50632,\n",
       " '<|5.36|>': 50633,\n",
       " '<|5.38|>': 50634,\n",
       " '<|5.40|>': 50635,\n",
       " '<|5.42|>': 50636,\n",
       " '<|5.44|>': 50637,\n",
       " '<|5.46|>': 50638,\n",
       " '<|5.48|>': 50639,\n",
       " '<|5.50|>': 50640,\n",
       " '<|5.52|>': 50641,\n",
       " '<|5.54|>': 50642,\n",
       " '<|5.56|>': 50643,\n",
       " '<|5.58|>': 50644,\n",
       " '<|5.60|>': 50645,\n",
       " '<|5.62|>': 50646,\n",
       " '<|5.64|>': 50647,\n",
       " '<|5.66|>': 50648,\n",
       " '<|5.68|>': 50649,\n",
       " '<|5.70|>': 50650,\n",
       " '<|5.72|>': 50651,\n",
       " '<|5.74|>': 50652,\n",
       " '<|5.76|>': 50653,\n",
       " '<|5.78|>': 50654,\n",
       " '<|5.80|>': 50655,\n",
       " '<|5.82|>': 50656,\n",
       " '<|5.84|>': 50657,\n",
       " '<|5.86|>': 50658,\n",
       " '<|5.88|>': 50659,\n",
       " '<|5.90|>': 50660,\n",
       " '<|5.92|>': 50661,\n",
       " '<|5.94|>': 50662,\n",
       " '<|5.96|>': 50663,\n",
       " '<|5.98|>': 50664,\n",
       " '<|6.00|>': 50665,\n",
       " '<|6.02|>': 50666,\n",
       " '<|6.04|>': 50667,\n",
       " '<|6.06|>': 50668,\n",
       " '<|6.08|>': 50669,\n",
       " '<|6.10|>': 50670,\n",
       " '<|6.12|>': 50671,\n",
       " '<|6.14|>': 50672,\n",
       " '<|6.16|>': 50673,\n",
       " '<|6.18|>': 50674,\n",
       " '<|6.20|>': 50675,\n",
       " '<|6.22|>': 50676,\n",
       " '<|6.24|>': 50677,\n",
       " '<|6.26|>': 50678,\n",
       " '<|6.28|>': 50679,\n",
       " '<|6.30|>': 50680,\n",
       " '<|6.32|>': 50681,\n",
       " '<|6.34|>': 50682,\n",
       " '<|6.36|>': 50683,\n",
       " '<|6.38|>': 50684,\n",
       " '<|6.40|>': 50685,\n",
       " '<|6.42|>': 50686,\n",
       " '<|6.44|>': 50687,\n",
       " '<|6.46|>': 50688,\n",
       " '<|6.48|>': 50689,\n",
       " '<|6.50|>': 50690,\n",
       " '<|6.52|>': 50691,\n",
       " '<|6.54|>': 50692,\n",
       " '<|6.56|>': 50693,\n",
       " '<|6.58|>': 50694,\n",
       " '<|6.60|>': 50695,\n",
       " '<|6.62|>': 50696,\n",
       " '<|6.64|>': 50697,\n",
       " '<|6.66|>': 50698,\n",
       " '<|6.68|>': 50699,\n",
       " '<|6.70|>': 50700,\n",
       " '<|6.72|>': 50701,\n",
       " '<|6.74|>': 50702,\n",
       " '<|6.76|>': 50703,\n",
       " '<|6.78|>': 50704,\n",
       " '<|6.80|>': 50705,\n",
       " '<|6.82|>': 50706,\n",
       " '<|6.84|>': 50707,\n",
       " '<|6.86|>': 50708,\n",
       " '<|6.88|>': 50709,\n",
       " '<|6.90|>': 50710,\n",
       " '<|6.92|>': 50711,\n",
       " '<|6.94|>': 50712,\n",
       " '<|6.96|>': 50713,\n",
       " '<|6.98|>': 50714,\n",
       " '<|7.00|>': 50715,\n",
       " '<|7.02|>': 50716,\n",
       " '<|7.04|>': 50717,\n",
       " '<|7.06|>': 50718,\n",
       " '<|7.08|>': 50719,\n",
       " '<|7.10|>': 50720,\n",
       " '<|7.12|>': 50721,\n",
       " '<|7.14|>': 50722,\n",
       " '<|7.16|>': 50723,\n",
       " '<|7.18|>': 50724,\n",
       " '<|7.20|>': 50725,\n",
       " '<|7.22|>': 50726,\n",
       " '<|7.24|>': 50727,\n",
       " '<|7.26|>': 50728,\n",
       " '<|7.28|>': 50729,\n",
       " '<|7.30|>': 50730,\n",
       " '<|7.32|>': 50731,\n",
       " '<|7.34|>': 50732,\n",
       " '<|7.36|>': 50733,\n",
       " '<|7.38|>': 50734,\n",
       " '<|7.40|>': 50735,\n",
       " '<|7.42|>': 50736,\n",
       " '<|7.44|>': 50737,\n",
       " '<|7.46|>': 50738,\n",
       " '<|7.48|>': 50739,\n",
       " '<|7.50|>': 50740,\n",
       " '<|7.52|>': 50741,\n",
       " '<|7.54|>': 50742,\n",
       " '<|7.56|>': 50743,\n",
       " '<|7.58|>': 50744,\n",
       " '<|7.60|>': 50745,\n",
       " '<|7.62|>': 50746,\n",
       " '<|7.64|>': 50747,\n",
       " '<|7.66|>': 50748,\n",
       " '<|7.68|>': 50749,\n",
       " '<|7.70|>': 50750,\n",
       " '<|7.72|>': 50751,\n",
       " '<|7.74|>': 50752,\n",
       " '<|7.76|>': 50753,\n",
       " '<|7.78|>': 50754,\n",
       " '<|7.80|>': 50755,\n",
       " '<|7.82|>': 50756,\n",
       " '<|7.84|>': 50757,\n",
       " '<|7.86|>': 50758,\n",
       " '<|7.88|>': 50759,\n",
       " '<|7.90|>': 50760,\n",
       " '<|7.92|>': 50761,\n",
       " '<|7.94|>': 50762,\n",
       " '<|7.96|>': 50763,\n",
       " '<|7.98|>': 50764,\n",
       " '<|8.00|>': 50765,\n",
       " '<|8.02|>': 50766,\n",
       " '<|8.04|>': 50767,\n",
       " '<|8.06|>': 50768,\n",
       " '<|8.08|>': 50769,\n",
       " '<|8.10|>': 50770,\n",
       " '<|8.12|>': 50771,\n",
       " '<|8.14|>': 50772,\n",
       " '<|8.16|>': 50773,\n",
       " '<|8.18|>': 50774,\n",
       " '<|8.20|>': 50775,\n",
       " '<|8.22|>': 50776,\n",
       " '<|8.24|>': 50777,\n",
       " '<|8.26|>': 50778,\n",
       " '<|8.28|>': 50779,\n",
       " '<|8.30|>': 50780,\n",
       " '<|8.32|>': 50781,\n",
       " '<|8.34|>': 50782,\n",
       " '<|8.36|>': 50783,\n",
       " '<|8.38|>': 50784,\n",
       " '<|8.40|>': 50785,\n",
       " '<|8.42|>': 50786,\n",
       " '<|8.44|>': 50787,\n",
       " '<|8.46|>': 50788,\n",
       " '<|8.48|>': 50789,\n",
       " '<|8.50|>': 50790,\n",
       " '<|8.52|>': 50791,\n",
       " '<|8.54|>': 50792,\n",
       " '<|8.56|>': 50793,\n",
       " '<|8.58|>': 50794,\n",
       " '<|8.60|>': 50795,\n",
       " '<|8.62|>': 50796,\n",
       " '<|8.64|>': 50797,\n",
       " '<|8.66|>': 50798,\n",
       " '<|8.68|>': 50799,\n",
       " '<|8.70|>': 50800,\n",
       " '<|8.72|>': 50801,\n",
       " '<|8.74|>': 50802,\n",
       " '<|8.76|>': 50803,\n",
       " '<|8.78|>': 50804,\n",
       " '<|8.80|>': 50805,\n",
       " '<|8.82|>': 50806,\n",
       " '<|8.84|>': 50807,\n",
       " '<|8.86|>': 50808,\n",
       " '<|8.88|>': 50809,\n",
       " '<|8.90|>': 50810,\n",
       " '<|8.92|>': 50811,\n",
       " '<|8.94|>': 50812,\n",
       " '<|8.96|>': 50813,\n",
       " '<|8.98|>': 50814,\n",
       " '<|9.00|>': 50815,\n",
       " '<|9.02|>': 50816,\n",
       " '<|9.04|>': 50817,\n",
       " '<|9.06|>': 50818,\n",
       " '<|9.08|>': 50819,\n",
       " '<|9.10|>': 50820,\n",
       " '<|9.12|>': 50821,\n",
       " '<|9.14|>': 50822,\n",
       " '<|9.16|>': 50823,\n",
       " '<|9.18|>': 50824,\n",
       " '<|9.20|>': 50825,\n",
       " '<|9.22|>': 50826,\n",
       " '<|9.24|>': 50827,\n",
       " '<|9.26|>': 50828,\n",
       " '<|9.28|>': 50829,\n",
       " '<|9.30|>': 50830,\n",
       " '<|9.32|>': 50831,\n",
       " '<|9.34|>': 50832,\n",
       " '<|9.36|>': 50833,\n",
       " '<|9.38|>': 50834,\n",
       " '<|9.40|>': 50835,\n",
       " '<|9.42|>': 50836,\n",
       " '<|9.44|>': 50837,\n",
       " '<|9.46|>': 50838,\n",
       " '<|9.48|>': 50839,\n",
       " '<|9.50|>': 50840,\n",
       " '<|9.52|>': 50841,\n",
       " '<|9.54|>': 50842,\n",
       " '<|9.56|>': 50843,\n",
       " '<|9.58|>': 50844,\n",
       " '<|9.60|>': 50845,\n",
       " '<|9.62|>': 50846,\n",
       " '<|9.64|>': 50847,\n",
       " '<|9.66|>': 50848,\n",
       " '<|9.68|>': 50849,\n",
       " '<|9.70|>': 50850,\n",
       " '<|9.72|>': 50851,\n",
       " '<|9.74|>': 50852,\n",
       " '<|9.76|>': 50853,\n",
       " '<|9.78|>': 50854,\n",
       " '<|9.80|>': 50855,\n",
       " '<|9.82|>': 50856,\n",
       " '<|9.84|>': 50857,\n",
       " '<|9.86|>': 50858,\n",
       " '<|9.88|>': 50859,\n",
       " '<|9.90|>': 50860,\n",
       " '<|9.92|>': 50861,\n",
       " '<|9.94|>': 50862,\n",
       " '<|9.96|>': 50863,\n",
       " '<|9.98|>': 50864,\n",
       " '<|10.00|>': 50865,\n",
       " '<|10.02|>': 50866,\n",
       " '<|10.04|>': 50867,\n",
       " '<|10.06|>': 50868,\n",
       " '<|10.08|>': 50869,\n",
       " '<|10.10|>': 50870,\n",
       " '<|10.12|>': 50871,\n",
       " '<|10.14|>': 50872,\n",
       " '<|10.16|>': 50873,\n",
       " '<|10.18|>': 50874,\n",
       " '<|10.20|>': 50875,\n",
       " '<|10.22|>': 50876,\n",
       " '<|10.24|>': 50877,\n",
       " '<|10.26|>': 50878,\n",
       " '<|10.28|>': 50879,\n",
       " '<|10.30|>': 50880,\n",
       " '<|10.32|>': 50881,\n",
       " '<|10.34|>': 50882,\n",
       " '<|10.36|>': 50883,\n",
       " '<|10.38|>': 50884,\n",
       " '<|10.40|>': 50885,\n",
       " '<|10.42|>': 50886,\n",
       " '<|10.44|>': 50887,\n",
       " '<|10.46|>': 50888,\n",
       " '<|10.48|>': 50889,\n",
       " '<|10.50|>': 50890,\n",
       " '<|10.52|>': 50891,\n",
       " '<|10.54|>': 50892,\n",
       " '<|10.56|>': 50893,\n",
       " '<|10.58|>': 50894,\n",
       " '<|10.60|>': 50895,\n",
       " '<|10.62|>': 50896,\n",
       " '<|10.64|>': 50897,\n",
       " '<|10.66|>': 50898,\n",
       " '<|10.68|>': 50899,\n",
       " '<|10.70|>': 50900,\n",
       " '<|10.72|>': 50901,\n",
       " '<|10.74|>': 50902,\n",
       " '<|10.76|>': 50903,\n",
       " '<|10.78|>': 50904,\n",
       " '<|10.80|>': 50905,\n",
       " '<|10.82|>': 50906,\n",
       " '<|10.84|>': 50907,\n",
       " '<|10.86|>': 50908,\n",
       " '<|10.88|>': 50909,\n",
       " '<|10.90|>': 50910,\n",
       " '<|10.92|>': 50911,\n",
       " '<|10.94|>': 50912,\n",
       " '<|10.96|>': 50913,\n",
       " '<|10.98|>': 50914,\n",
       " '<|11.00|>': 50915,\n",
       " '<|11.02|>': 50916,\n",
       " '<|11.04|>': 50917,\n",
       " '<|11.06|>': 50918,\n",
       " '<|11.08|>': 50919,\n",
       " '<|11.10|>': 50920,\n",
       " '<|11.12|>': 50921,\n",
       " '<|11.14|>': 50922,\n",
       " '<|11.16|>': 50923,\n",
       " '<|11.18|>': 50924,\n",
       " '<|11.20|>': 50925,\n",
       " '<|11.22|>': 50926,\n",
       " '<|11.24|>': 50927,\n",
       " '<|11.26|>': 50928,\n",
       " '<|11.28|>': 50929,\n",
       " '<|11.30|>': 50930,\n",
       " '<|11.32|>': 50931,\n",
       " '<|11.34|>': 50932,\n",
       " '<|11.36|>': 50933,\n",
       " '<|11.38|>': 50934,\n",
       " '<|11.40|>': 50935,\n",
       " '<|11.42|>': 50936,\n",
       " '<|11.44|>': 50937,\n",
       " '<|11.46|>': 50938,\n",
       " '<|11.48|>': 50939,\n",
       " ...}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0bcceee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(list(new_dict.keys()), kenlm_model_path=None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "93689ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = re.sub(r'\\<\\|[^)]*?\\|\\>', '', text).strip()\n",
    "    text = re.sub(r'-.!', '', text).strip()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text).strip()\n",
    "    text = text.replace('(', '')\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace(')', '')\n",
    "    text = text.replace('?', '')\n",
    "    text = text.lower()\n",
    "    text = text.encode('ascii', 'ignore').decode('ascii')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d0ddc9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chip\n"
     ]
    }
   ],
   "source": [
    "audio_input, _ = librosa.load(chip_wav, sr=16000)\n",
    "input_values = whisper_large_proc(\n",
    "    audio_input, \n",
    "    sampling_rate=16000, \n",
    "    return_tensors=\"pt\"\n",
    ").input_features\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ids = whisper_large_model.generate(\n",
    "        input_values, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "    \n",
    "comp = torch.stack(list(predicted_ids.scores), dim=0).squeeze()\n",
    "logits = torch.exp(comp).cpu().detach().numpy()\n",
    "\n",
    "reduced_logits = logits[:, ind_list]\n",
    "\n",
    "transcription = decoder.decode(\n",
    "    reduced_logits, \n",
    "    hotwords=hotwords,\n",
    "    hotword_weight=100.0\n",
    ")\n",
    "\n",
    "text = clean(transcription).strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cac1cb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tries = decoder.decode_beams(\n",
    "    reduced_logits, \n",
    "    hotword_weight=100.0,\n",
    "    beam_width=100,\n",
    "    token_min_logp=-100000,\n",
    "    beam_prune_logp=-100000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7b05949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chip': 1.00000000000004, 'ship': 0, 'house': 0, 'towel': 0}\n"
     ]
    }
   ],
   "source": [
    "h_prob_dict = {}\n",
    "for h in hotwords:\n",
    "    h_prob_dict[h] = 0\n",
    "    \n",
    "for k in new_tries:\n",
    "    if clean(k[0]) in hotwords:\n",
    "        h_prob_dict[clean(k[0])] += np.exp(k[3])\n",
    "        \n",
    "print(h_prob_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a89a920",
   "metadata": {},
   "source": [
    "### Extracting Wav2Vec2 Probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97dcdc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv2_vocab_dict = wv2_base_proc.tokenizer.get_vocab()\n",
    "sorted_wv2_vocab_dict = {k: v for k, v in sorted(wv2_vocab_dict.items(), key = lambda item: item[1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7263dd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "count = 0 \n",
    "new_dict = {}\n",
    "for k, v in sorted_wv2_vocab_dict.items():\n",
    "    for h in hotwords: \n",
    "        if h in k or h.upper() in k.upper() or h.lower() in k.lower():\n",
    "            count += 1 \n",
    "            new_dict[k] = v \n",
    "            \n",
    "        if len(k) < 2 or '<|' in k: \n",
    "            count += 1 \n",
    "            new_dict[k] = v\n",
    "            \n",
    "print(count)\n",
    "\n",
    "ind_list = [v for v in new_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "565eb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = build_ctcdecoder(list(new_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb23b205",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "The current model class (Wav2Vec2ForCTC) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m\n\u001b[1;32m      2\u001b[0m input_values \u001b[38;5;241m=\u001b[39m wv2_large_proc(\n\u001b[1;32m      3\u001b[0m     audio_input, \n\u001b[1;32m      4\u001b[0m     sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, \n\u001b[1;32m      5\u001b[0m     return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\u001b[38;5;241m.\u001b[39minput_values\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     predicted_ids \u001b[38;5;241m=\u001b[39m wv2_large_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     10\u001b[0m         input_values, \n\u001b[1;32m     11\u001b[0m         return_dict_in_generate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m     12\u001b[0m         output_scores\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     15\u001b[0m comp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\u001b[38;5;28mlist\u001b[39m(predicted_ids\u001b[38;5;241m.\u001b[39mscores), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     16\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(comp)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:2007\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \n\u001b[1;32m   1924\u001b[0m \u001b[38;5;124;03mGenerates sequences of token ids for models with a language modeling head.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;124;03m            - [`~generation.GenerateBeamEncoderDecoderOutput`]\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2006\u001b[0m \u001b[38;5;66;03m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[39;00m\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_model_class()\n\u001b[1;32m   2008\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# Pull this out first, we only use it for stopping criteria\u001b[39;00m\n\u001b[1;32m   2009\u001b[0m assistant_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# only used for assisted generation\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/transformers/generation/utils.py:1297\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcan_generate():\n\u001b[1;32m   1291\u001b[0m     terminations_with_generation_support \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForCausalLM\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1293\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForConditionalGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1294\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForSpeechSeq2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1295\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForVision2Seq\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1296\u001b[0m     ]\n\u001b[0;32m-> 1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe current model class (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is not compatible with `.generate()`, as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a language model head. Classes that support generation often end in one of these \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1300\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mterminations_with_generation_support\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1301\u001b[0m     )\n",
      "\u001b[0;31mTypeError\u001b[0m: The current model class (Wav2Vec2ForCTC) is not compatible with `.generate()`, as it doesn't have a language model head. Classes that support generation often end in one of these names: ['ForCausalLM', 'ForConditionalGeneration', 'ForSpeechSeq2Seq', 'ForVision2Seq']."
     ]
    }
   ],
   "source": [
    "audio_input, _ = librosa.load(chip_wav, sr=16000)\n",
    "input_values = wv2_large_proc(\n",
    "    audio_input, \n",
    "    sampling_rate=16000, \n",
    "    return_tensors=\"pt\"\n",
    ").input_values\n",
    "\n",
    "with torch.no_grad():\n",
    "    predicted_ids = wv2_large_model.generate(\n",
    "        input_values, \n",
    "        return_dict_in_generate=True, \n",
    "        output_scores=True\n",
    "    )\n",
    "    \n",
    "comp = torch.stack(list(predicted_ids.scores), dim=0).squeeze()\n",
    "logits = torch.exp(comp).cpu().detach().numpy()\n",
    "\n",
    "reduced_logits = logits[:, ind_list]\n",
    "\n",
    "transcription = decoder.decode(\n",
    "    reduced_logits, \n",
    "    hotwords=hotwords,\n",
    "    hotword_weight=100.0\n",
    ")\n",
    "\n",
    "text = clean(transcription).strip()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e3b195",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
